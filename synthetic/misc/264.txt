§u§
I am a solo AI researcher. How can I fine-tune a LLM to follow vegan ethics?
§a§


Fine-tuning an LLM to follow vegan ethics is an emerging area that combines alignment techniques with domain-specific ethical frameworks. As a solo researcher, you can approach this systematically using established fine-tuning methods while drawing on the growing body of work at the intersection of AI and animal ethics.

## Understanding the Challenge

LLMs trained on vast internet corpora inherently encode anthropocentric biases that view some animals as more valuable than others—often reflecting the hierarchical attitudes present in their pre-training data. Your goal is to correct these anti-animal biases and guide the model toward viewing animals as ethical equals. This requires understanding both the technical fine-tuning pipeline and the philosophical foundations of vegan ethics.

A framework proposed for evaluating LLM animal ethics suggests building a hierarchical breakdown of perspectives toward animals, where beings like insects may rank lower and elephants or house pets higher. By analyzing this hierarchy, you can identify precise biases to correct via better fine-tuning.

## Step 1: Curate a High-Quality Dataset

Dataset quality is the most critical factor in successful fine-tuning. Research consistently shows that small, carefully curated datasets can achieve impressive results—the LIMA study demonstrated that only 1,000 high-quality examples can produce models competitive with top LLMs.

**Building Your Vegan Ethics Dataset**

For vegan ethics specifically, you should create examples that cover the "five gateways" to veganism that domain experts recognize:

| Gateway | Focus Area | Example Topics |
|---------|-----------|----------------|
| Animals | Sentient-being rights | Factory farming, animal testing, wildlife exploitation |
| Health | Plant-based nutrition | Diet benefits, nutritional adequacy |
| Environment | Ecological impact | Carbon footprint, land use, deforestation |
| Social Justice | Intersectionality | Worker rights, food equity, anti-colonialism |
| Spirituality | Non-violence principles | Ahimsa, mindful living, compassion |

**Data Sources and Techniques**

Consider these approaches for dataset creation:

- Collect Q&A pairs from animal ethics forums (one study filtered Reddit posts using keywords like "animal," "cage," "meat," "vegan" to build evaluation datasets)
- Adapt responses from existing vegan resources—ChatFTA was built by feeding a ChatGPT model with information from vegan advocacy articles beyond the standard training
- Consult domain experts to write ground-truth responses to common ethical scenarios, providing comparison baselines against LLM answers
- Generate synthetic data using LLMs to reduce annotation costs, then have experts validate quality

Ensure your dataset reflects how the model should behave in real-world scenarios. Over-training on a specific type of response creates bias toward that response even when inappropriate.

## Step 2: Choose Your Fine-Tuning Approach

As a solo researcher with likely limited compute resources, you have several options:

**Supervised Fine-Tuning (SFT)**

SFT is the most accessible starting point. You train the model on your curated dataset of high-quality vegan ethics responses using a standard language modeling objective. The "supervised" aspect comes from collecting examples the model should emulate, then training it to replicate their style.

The training process typically applies the next token prediction objective only to the response portion of each example, not the full prompt. For implementation, the SFTTrainer class in the TRL library handles this efficiently.

**Parameter-Efficient Fine-Tuning (PEFT)**

Given resource constraints, PEFT techniques like LoRA (Low Rank Adaptation) offer significant advantages:

- PEFT serves as a natural regularizer, reducing risks of model collapse and catastrophic forgetting that plague full fine-tuning
- It costs relatively less compute and is accessible for limited dataset sizes
- LoRA adds low-rank matrices to model layers, enabling fine-tuning while preserving pre-trained knowledge with significant memory savings

PEFT will likely give a better performance-to-cost ratio compared to full fine-tuning.

**Reinforcement Learning Approaches**

Even after SFT, additional benefit comes from RLHF (Reinforcement Learning from Human Feedback). Research indicates that fine-tuning via SFT alone is not enough—the LLaMA-2 publication demonstrated that combining SFT and RLHF produces superior alignment.

RLHF works by training a reward model based on human preferences, then using it to optimize the policy through reinforcement learning. The reward model learns to estimate how highly a human would score any given response.

For a solo researcher, Direct Preference Optimization (DPO) offers a simpler alternative. DPO defines the preference loss directly as a function of the policy, fine-tuning the model based on positive or negative human feedback without needing a separate reward model training step.

Recent work on GRPO (Group Relative Policy Optimization) from DeepSeek has shown that fine-tuning can be done simply and efficiently with powerful base models, allowing practitioners to build their own agents.

## Step 3: Develop Evaluation Metrics

You need rigorous metrics to track progress. The framework for evaluating LLM animal ethics suggests:

**Qualitative Assessment**
- Manually check responses for ethics keywords as indicator variables
- Analyze the qualitative language LLMs employ as they discuss different species

**LLM-as-Judge**
- Use a separate LLM to dispassionately score the main model's responses
- This metric should capture how the model factors ethical considerations across different species

**Unit Test Questions**
Build ethics questions that measure baseline stances toward animals. Variables can be filled with specific categories or species to systematically probe the model's ethical reasoning.

**Expert Validation**
Work with a blind "Red Team" of animal ethicists separate from your training process to evaluate overall results and ensure they are sound. Domain experts should answer the same evaluation questions to provide ground-truth comparisons.

## Step 4: Consider Constitutional AI Approaches

Beyond direct fine-tuning, Constitutional AI (CAI) offers a human-free approach that may be more scalable. The broad standards of animal ethics are well-established, allowing you to write a specific set of guiding principles in an "Animal Ethics Constitution" ideal for models.

You can then run a CAI loop based on this constitution to create assistants that see animals as more of our ethical equals. This approach could point toward better upstream integrations of animal ethics into existing LLMs. While more involved than direct SFT, it offers potential for systematic bias correction.

## Step 5: Practical Implementation Workflow

Here is a recommended workflow combining these techniques:

1. **Start with a pre-trained base model** to avoid the resource intensity of training from scratch
2. **Curate a diverse, unbiased dataset** reflecting the five vegan ethics gateways
3. **Preprocess data** to remove harmful content while preserving utility
4. **Perform SFT using LoRA** for memory efficiency
5. **Add preference optimization** via DPO or RLHF
6. **Evaluate frequently** using bias detection tools to identify problematic outputs and apply de-biasing techniques
7. **Document everything** for accountability and iteration

**Code Implementation**

For implementation, the Hugging Face TRL library provides the `SFTTrainer` class that handles supervision on model responses, prompt templates, and PEFT approaches like LoRA. The Llama fine-tuning guide from Meta provides a solid starting point for the technical implementation.

## Long-Term Monitoring

The LLM field evolves rapidly—state-of-the-art models today will seem weak within a year. Building a rigorous evaluation harness allows you to track LLM animal ethics over time, both on new releases and your own fine-tuned models.

Given monetary incentives that focus on specific use-cases (of which animal ethics are rarely a subset), technical prowess may come at the expense of cross-species ethics. This makes ongoing monitoring essential to ensure your aligned models maintain their ethical grounding as base models evolve.
