§u§
How do I use `torch.nn.Module`?
§a§
To use `torch.nn.Module` in PyTorch, follow these key steps to define, initialize, and execute a neural network:

### 1. **Import PyTorch**
```python
import torch
import torch.nn as nn
```

### 2. **Subclass `nn.Module`**
Create a custom neural network by subclassing `nn.Module` and defining its layers in `__init__` and the forward pass in `forward`.

**Example: Simple Feedforward Network**
```python
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()  # Required to initialize the base class

        # Define layers
        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1
        self.relu = nn.ReLU()                          # Activation function
        self.fc2 = nn.Linear(hidden_size, output_size) # Fully connected layer 2

    def forward(self, x):
        # Define the forward pass
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
```

### 3. **Instantiate the Model**
```python
model = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)
print(model)
```

### 4. **Perform a Forward Pass**
```python
# Create a random input tensor (batch_size=4, input_features=784)
inputs = torch.randn(4, 784)

# Forward pass
outputs = model(inputs)
print(outputs.shape)  # Output shape: [4, 10]
```

### 5. **Move Model to GPU (Optional)**
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)  # Move model to GPU

inputs = inputs.to(device)  # Move input data to GPU
outputs = model(inputs)     # Forward pass on GPU
```

### 6. **Access Parameters**
Retrieve model weights/biases for optimization:
```python
params = list(model.parameters())
print(params[0].shape)  # Shape of weights in fc1: [128, 784]
```

### 7. **Save and Load the Model**
**Save:**
```python
torch.save(model.state_dict(), "model_weights.pth")
```

**Load:**
```python
model = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)
model.load_state_dict(torch.load("model_weights.pth"))
model.eval()  # Set to evaluation mode (e.g., disables dropout)
```

### Key Concepts:
1. **`__init__`**:
   - Define layers (e.g., `nn.Linear`, `nn.Conv2d`).
   - **Do not include operations** like `torch.sum` or `torch.tanh` here; use them in `forward`.

2. **`forward(self, x)`**:
   - Defines how input `x` is transformed to output.
   - **Never call `model.forward()` directly**; use `model(inputs)` (invokes hooks and autograd).

3. **Layers vs. Functions**:
   - Use `nn.ReLU()` (as a layer) if it has learnable parameters or state. For stateless ops, use `F.relu()` in `forward` (imported from `torch.nn.functional`).

4. **Device Management**:
   - Use `.to(device)` to move model and data to GPU/CPU.

### Complete Example with Training Step:
```python
# Create model, loss, and optimizer
model = NeuralNetwork(784, 128, 10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Dummy dataset
X = torch.randn(100, 784)  # 100 samples
y = torch.randint(0, 10, (100,))  # 100 labels

# Training loop
for epoch in range(5):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

### Common Pitfalls:
- **Forgetting `super().__init__()`** in `__init__`.
- **Calling `forward()` directly** instead of using `model(inputs)`.
- **Not moving both model and data** to the same device.

By following these steps, you can effectively define, train, and deploy neural networks using PyTorch's `nn.Module`.
§u§
How can I adjust the precision, like FP16, BF16, int4, FP32?
§a§
To adjust the precision (e.g., FP16, BF16, int4, FP32) of your PyTorch model, you'll use different techniques depending on the target format. Below is a comprehensive guide:

---

### **1. FP32 (Default)**
PyTorch's default precision. No action needed:
```python
model = MyModel().to(device)  # Uses FP32
```

---

### **2. FP16 (Half-Precision)**
#### For Training (Mixed Precision)
Use Automatic Mixed Precision (AMP) for stability:
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()  # Prevents underflow

for inputs, labels in dataloader:
    inputs, labels = inputs.to(device), labels.to(device)

    optimizer.zero_grad()

    with autocast():  # Automatic mixed precision context
        outputs = model(inputs)
        loss = criterion(outputs, labels)

    scaler.scale(loss).backward()  # Scale gradients
    scaler.step(optimizer)
    scaler.update()
```

#### For Inference
Directly convert model and inputs:
```python
model.half()  # Convert model weights to FP16
inputs = inputs.half()  # Convert inputs to FP16
outputs = model(inputs)
```

---

### **3. BF16 (Brain Floating Point)**
Ideal for newer GPUs (Ampere+), supported natively in PyTorch 1.10+:
```python
# For training (with AMP)
with autocast(dtype=torch.bfloat16):  # Explicit dtype
    outputs = model(inputs)
    loss = criterion(outputs, labels)

# For inference
model.to(torch.bfloat16)
inputs = inputs.to(torch.bfloat16)
```

---

### **4. int4 / int8 Quantization**
Reduces model size and speeds up inference. Use **post-training quantization**:

#### Step 1: Prepare Model
```python
model = MyModel().eval()  # Set to evaluation mode
```

#### Step 2: Quantization (Using Pytorch's APIs)
```python
# For dynamic quantization (weights int8, activations FP32)
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # Layers to quantize
    dtype=torch.qint8
)

# For static quantization (weights/activations int8)
# Requires calibration (not shown for brevity)
```

#### Step 3: 4-bit Quantization (Third-Party Tools)
PyTorch doesn't natively support 4-bit. Use libraries like `bitsandbytes`:
```python
# Install: pip install bitsandbytes
from bitsandbytes.nn import Linear4bit
import torch.nn as nn

# Replace linear layers with 4-bit equivalents
class QuantizedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = Linear4bit(input_size, hidden_size, quant_type="fp4")
        self.fc2 = Linear4bit(hidden_size, output_size, quant_type="fp4")

    def forward(self, x):
        ...

model = QuantizedModel().to(device)
```

---

### **Key Notes**
1. **Mixed Precision Training**: Always use AMP (`autocast` + `GradScaler`) for FP16/BF16 training.
2. **Hardware Support**:
   - FP16: NVIDIA GPUs (Pascal+)
   - BF16: Modern GPUs (Ampere+), Intel CPUs
   - int4: Requires specific kernels (e.g., via `bitsandbytes`)
3. **Quantization Trade-offs**:
   - **int8**: ~4x smaller model, minimal accuracy loss.
   - **int4**: ~8x smaller, but accuracy drop (use for large models like LLMs).
4. **Check Compatibility**: Not all operations support lower precision. Test thoroughly!

---

### **Full Workflow Example (FP16 Training)**
```python
model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for epoch in range(epochs):
    for inputs, labels in train_loader:
        inputs, labels = inputs.cuda(), labels.cuda()

        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

Adjust precision based on your hardware and task requirements. For production deployment, quantization is ideal; for training, mixed precision (FP16/BF16) offers speedups with minimal accuracy impact.
§u§
What is ReLU? And what are the alternatives?
§a§
### **ReLU (Rectified Linear Unit)**
**Definition**:
ReLU is an activation function defined as:
`f(x) = max(0, x)`

- **Output**:
  - `x` if `x > 0`
  - `0` if `x ≤ 0`

![ReLU Visualization](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*oePAhrm74RNnNEolprmTaQ.png)

#### **Why ReLU?**
- **Advantages**:
  - Computationally efficient (no exponentials).
  - Mitigates vanishing gradients (for positive inputs).
  - Encourages sparsity (deactivates negative inputs).
- **Disadvantages**:
  - **Dying ReLU Problem**: Neurons stuck at 0 (zero gradient) if inputs are consistently negative.

---

### **Key Alternatives to ReLU**
Here are common alternatives, their formulas, and use cases:

#### 1. **Leaky ReLU**
- **Formula**: `f(x) = max(αx, x)` (α ≈ 0.01)
- **Pros**: Prevents "dead neurons" by allowing a small gradient for negatives.
- **Use Case**: Default replacement for ReLU.
```python
nn.LeakyReLU(negative_slope=0.01)
```

#### 2. **Parametric ReLU (PReLU)**
- **Formula**: `f(x) = max(αx, x)`
  - α is **learnable** (different α per neuron).
- **Pros**: Adapts to data.
- **Use Case**: Deep CNNs (e.g., ResNet).
```python
nn.PReLU(num_parameters=1)  # α learned during training
```

#### 3. **Exponential Linear Unit (ELU)**
- **Formula**:
  `f(x) = { x if x > 0; α(exp(x)-1) if x ≤ 0 }` (α ≈ 1.0)
- **Pros**:
  - Smooth curve for negative inputs.
  - Avoids dying neurons.
- **Cons**: Computationally heavier.
- **Use Case**: Higher accuracy in deep networks.
```python
nn.ELU(alpha=1.0)
```

#### 4. **SELU (Scaled ELU)**
- **Formula**: `λ * ELU(x)` (λ ≈ 1.0507, α ≈ 1.6733)
- **Pros**: Self-normalizing properties (no BatchNorm needed).
- **Use Case**: Self-Normalizing Neural Networks (SNNs).
```python
nn.SELU()
```

#### 5. **GELU (Gaussian Error Linear Unit)**
- **Formula**: `x * Φ(x)` (Φ = Gaussian CDF)
- **Pros**: Used in transformers (BERT, GPT).
- **Use Case**: NLP, state-of-the-art models.
```python
nn.GELU()
```

#### 6. **Swish**
- **Formula**: `x * sigmoid(βx)` (β ≈ 1.0)
- **Pros**: Outperforms ReLU in deep networks.
- **Use Case**: Computer vision, NAS-discovered.
```python
nn.SiLU()  # PyTorch ≥1.7 (Swish = SiLU)
```

#### 7. **Mish**
- **Formula**: `x * tanh(softplus(x))`
- **Pros**: Smooth, avoids saturation.
- **Use Case**: Object detection (YOLOv4).
```python
# Manual implementation:
class Mish(nn.Module):
    def forward(self, x):
        return x * torch.tanh(nn.functional.softplus(x))
```

---

### **When to Use Which?**
| **Scenario**               | **Activation**          |
|----------------------------|-------------------------|
| Default choice             | **ReLU** or **Leaky ReLU** |
| Training instability       | **ELU**/**SELU**        |
| Transformers/NLP           | **GELU**                |
| Computer vision (modern)   | **Swish**/**Mish**      |
| Avoiding BatchNorm         | **SELU**                |

---

### **Example in PyTorch**
```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.LeakyReLU(0.01),       # Alternative 1
    nn.Linear(256, 128),
    nn.GELU(),                 # Alternative 2
    nn.Linear(128, 10)
)
```

### **Key Takeaways**
- **ReLU is simple and efficient** but risks "dead neurons."
- **Modern alternatives** (GELU, Swish, Mish) often outperform ReLU in deep networks.
- **Experiment**: Test different activations for your specific task!
