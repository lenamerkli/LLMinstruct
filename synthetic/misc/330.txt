§u§
python download large file to a specific directory
§a§
To download a large file to a specific directory in Python efficiently (with streaming to handle large files), follow these steps:

### Solution Code
```python
import requests
import os
from pathlib import Path

def download_large_file(url: str, directory: str, filename: str = None):
    """
    Downloads a large file from a URL to a specified directory with streaming.

    Args:
        url: URL of the file to download
        directory: Target directory path (created if not exists)
        filename: Optional custom filename (uses original name if None)
    """
    # Create directory if missing
    Path(directory).mkdir(parents=True, exist_ok=True)

    # Get filename from URL if not provided
    if filename is None:
        filename = url.split('/')[-1].split('?')[0]  # Remove URL params

    filepath = os.path.join(directory, filename)

    # Stream download
    with requests.get(url, stream=True) as r:
        r.raise_for_status()  # Check for HTTP errors
        with open(filepath, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return filepath

# Example usage
if __name__ == "__main__":
    url = "https://example.com/largefile.zip"
    download_dir = "./downloaded_files"
    downloaded_path = download_large_file(url, download_dir)
    print(f"File saved to: {downloaded_path}")
```

### Key Features:
1. **Streaming Download**: Uses `requests.get(stream=True)` to avoid loading the entire file into memory.
2. **Directory Handling**: Automatically creates the target directory (including parent directories) if missing.
3. **Filename Extraction**: Defaults to the filename from the URL (strips URL parameters).
4. **Chunked Writing**: Writes data in 8KB chunks (adjustable) to minimize memory usage.
5. **Error Handling**: Checks HTTP status with `raise_for_status()`.

### Usage:
1. **Basic**
   `download_large_file("https://example.com/bigfile.iso", "/data/downloads")`

2. **Custom Filename**
   `download_large_file("https://example.com/data", "/backups", "archive.tar.gz")`

### Notes:
- Install required package: `pip install requests`
- Adjust `chunk_size` (currently 8192 bytes) based on your needs.
- Handles redirects automatically (via `requests`).
- Add try/except blocks around the call if you need to handle specific errors.
§u§
Will this work with files of 100GB?
§a§
Yes, the provided code will work for downloading a 100GB file, as it uses **streaming download** which handles large files efficiently. Here's why it's suitable and how to optimize it further:

### Key Features for Large Files:
1. **Streaming Download**:
   - Downloads in chunks (8KB by default) instead of loading the entire file into memory
   - Uses `iter_content()` to process data incrementally
   - Memory usage remains constant regardless of file size

2. **Disk Efficiency**:
   - Writes directly to disk with buffered writes (`wb` mode)
   - No intermediate storage in RAM

### Potential Improvements for 100GB Files:
```python
import requests
import os
from pathlib import Path

def download_large_file(
    url: str,
    directory: str,
    filename: str = None,
    chunk_size: int = 2 * 1024 * 1024,  # 2MB chunks
    timeout: tuple = (10, 30)  # (connect, read) timeouts
) -> str:
    """
    Downloads large files efficiently with improved handling for 100GB+ files

    Args:
        url: File URL
        directory: Target directory
        filename: Optional custom filename
        chunk_size: Chunk size in bytes (default: 2MB)
        timeout: Connection/read timeouts in seconds
    """
    Path(directory).mkdir(parents=True, exist_ok=True)

    # Extract filename from URL if not provided
    if not filename:
        filename = os.path.basename(requests.utils.urlparse(url).path) or "downloaded_file"

    filepath = os.path.join(directory, filename)

    # Stream download with timeout
    with requests.get(url, stream=True, timeout=timeout) as r:
        r.raise_for_status()
        with open(filepath, 'wb') as f:
            for chunk in r.iter_content(chunk_size=chunk_size):
                if chunk:  # Filter out keep-alive chunks
                    f.write(chunk)
    return filepath
```

### Critical Enhancements for 100GB Files:
1. **Increased Chunk Size**:
   ```python
   chunk_size = 2 * 1024 * 1024  # 2MB instead of 8KB
   ```
   - Reduces disk I/O operations by 256x
   - Optimizes for modern SSDs/HDDs that handle larger blocks better

2. **Timeout Handling**:
   ```python
   timeout = (10, 30)  # (connect timeout, read timeout)
   ```
   - Prevents indefinite hangs (critical for large downloads)
   - Separate timeouts for connection establishment and data transfer

3. **Robust Filename Parsing**:
   ```python
   filename = os.path.basename(requests.utils.urlparse(url).path) or "downloaded_file"
   ```
   - Handles complex URLs properly
   - Fallback for URLs without filename

### Usage Example:
```python
# Download 100GB file
download_large_file(
    url="https://example.com/100gb_dataset.zip",
    directory="/mnt/big_storage/downloads",
    chunk_size=5*1024*1024,  # 5MB chunks
    timeout=(15, 300)  # 15s connect, 5min read timeout
)
```

### Additional Recommendations for Production:
1. **Resume Capability**:
   - Implement range requests (`Accept-Ranges`) for resuming interrupted downloads
   - Check existing file size before downloading

2. **Progress Monitoring**:
   ```python
   total_size = int(r.headers.get('content-length', 0))
   downloaded = 0
   for chunk in r.iter_content(chunk_size=chunk_size):
       downloaded += len(chunk)
       print(f"Progress: {downloaded/total_size:.1%}", end='\r')
   ```

3. **Error Resilience**:
   - Add retry logic with backoff
   - Handle transient network errors
   - Verify file integrity with checksums

4. **Storage Considerations**:
   - Ensure destination filesystem supports >100GB files (e.g., NTFS, ext4, XFS)
   - Confirm adequate disk space before downloading

This implementation will successfully handle 100GB files while maintaining low memory usage (typically <100MB RAM) and providing robust error handling for production environments.
